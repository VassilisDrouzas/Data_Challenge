{"cells":[{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1993,"status":"ok","timestamp":1713897479834,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"gNWVf8O_yAmk","outputId":"baed6904-c5e6-406a-e955-c38b0da759e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":12016,"status":"ok","timestamp":1713897491848,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"BhK_1Irupz5q"},"outputs":[],"source":["%%capture\n","!pip install optuna"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713897491848,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"zj2IULDHlLoZ"},"outputs":[],"source":["import csv\n","import networkx as nx\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","import optuna\n","from sklearn.model_selection import cross_val_score, KFold\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20085,"status":"ok","timestamp":1713897511930,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"kJNPNK60lLop","outputId":"4e98542f-7219-4e86-8a02-d04badadf119"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of nodes: 65208\n","Number of edges: 1642073\n"]}],"source":["# file_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"data_challenge\", \"train.txt\")\n","\n","# read training data\n","train_domains = list()\n","y_train = list()\n","# with open(\"/content/drive/Othercomputers/My Computer/Masters_Staff/trimester_3/Data_challenge/project/data/train.txt\", 'r') as f:\n","with open(\"../../../../../data_challenge/train.txt\", 'r') as f:    \n","    for line in f:\n","        l = line.split(',') # domain names\n","        train_domains.append(l[0])\n","        y_train.append(l[1][:-1]) # topics of domain names\n","\n","# read test data\n","test_domains = list()\n","# with open(\"/content/drive/Othercomputers/My Computer/Masters_Staff/trimester_3/Data_challenge/project/data/test.txt\", 'r') as f:\n","with open(\"../../../../../data_challenge/test.txt\", 'r') as f:    \n","    for line in f:\n","        l = line.split(',')\n","        test_domains.append(l[0])\n","\n","# create a directed graph\n","# G = nx.read_edgelist('/content/drive/Othercomputers/My Computer/Masters_Staff/trimester_3/Data_challenge/project/data/edgelist.txt', delimiter=' ',\n","#                       create_using=nx.DiGraph())\n","\n","G = nx.read_edgelist('../../../../../data_challenge/edgelist.txt', delimiter=' ',\n","                      create_using=nx.DiGraph())\n","\n","print('Number of nodes:', G.number_of_nodes())\n","print('Number of edges:', G.number_of_edges())"]},{"cell_type":"markdown","metadata":{"id":"Aio1Q7eRoJXb"},"source":["## Read textual data from domains"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":124834,"status":"ok","timestamp":1713897636762,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"p11cWO8tnuMw"},"outputs":[],"source":["import csv\n","import re\n","import zipfile\n","from io import BytesIO\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","\n","# read textual content of webpages of domain names\n","text = dict()\n","# with zipfile.ZipFile('/content/drive/Othercomputers/My Computer/Masters_Staff/trimester_3/Data_challenge/project/data/domains.zip', \"r\") as zfile:\n","    \n","with zipfile.ZipFile('../../../../../data_challenge/domains.zip', \"r\") as zfile:\n","    for filename in zfile.namelist():\n","        if re.search(r'\\.zip$', filename) is not None:\n","            zfiledata = BytesIO(zfile.read(filename))\n","            with zipfile.ZipFile(zfiledata) as zfile2:\n","                text[filename[:-4]] = ''\n","                for name2 in zfile2.namelist():\n","                    file = zfile2.read(name2)\n","                    text[filename[:-4]] += file.decode('utf16') + ' '\n","\n","# retrieve textual content of domain names of the training set\n","train_data = list()\n","for domain in train_domains:\n","    if domain in text:\n","        train_data.append(text[domain])\n","    else:\n","        train_data.append('')\n","\n","# retrieve textual content of domain names of the test set\n","test_data = list()\n","for domain in test_domains:\n","    if domain in text:\n","        test_data.append(text[domain])\n","    else:\n","        test_data.append('')\n","\n","# to reduce memory\n","text = None"]},{"cell_type":"markdown","metadata":{"id":"JDli6UunlLoy"},"source":["### Degree Centrality\n","Degree centrality is a measure of the importance of a node within a network. It is simply the number of edges connected to a node, normalized by the maximum possible degree of the node"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713897636763,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"1unIosaclLo5","outputId":"c0a642cc-c51c-4fda-94ed-c0f724255b96"},"outputs":[{"name":"stdout","output_type":"stream","text":["blog.com.gr : 0.0007821246185225512\n","fmvoice.gr : 0.0038952873157789805\n","papakishop.gr : 0.0012421979235358166\n","rizospastis.gr : 0.032496511110770315\n","taxheaven.gr : 0.022436241507813575\n"]}],"source":["deg_central = nx.degree_centrality(G)\n","for key, value in list(deg_central.items())[:5]:\n","    print(key, \":\", value)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":320,"status":"ok","timestamp":1713897637081,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"9tNLEc47lLo9","outputId":"c382596b-7728-41ba-a422-98364b12f2a1"},"outputs":[],"source":["sorted(deg_central.items(), key= lambda kv: -kv[1]) # sort backwards"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1713897637081,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"vZuSomWhlLo-","outputId":"7656671c-9d05-4db4-9113-fbfc8a95389d"},"outputs":[{"name":"stdout","output_type":"stream","text":["['0', '5', '5', '0', '3']\n","['startupper.gr']\n","['autocarnet.gr']\n"]}],"source":["print(y_train[:5]) # topics of each of the labeled domain namess\n","print(test_domains[:1])\n","print(train_domains[:1])"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":237,"status":"ok","timestamp":1713897637317,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"GuK3pA5ZlLpC"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# create the training matrix. Each row corresponds to a web host.\n","# use the following 3 features for each web host:\n","# (1) int-degree of node\n","# (2) out-degree of node\n","# (3) average degree of neighborhood of node\n","X_train = np.zeros((len(train_domains), 3))\n","avg_neig_deg = nx.average_neighbor_degree(G, nodes=train_domains)\n","for i in range(len(train_domains)):\n","    X_train[i,0] = G.in_degree(train_domains[i])\n","    X_train[i,1] = G.out_degree(train_domains[i])\n","    X_train[i,2] = avg_neig_deg[train_domains[i]]\n","    # X_train[i,3] = round(deg_central_train[train_domains[i]], 5)\n","\n","# create the test matrix. Use the same 3 features as above\n","Xtest = np.zeros((len(test_domains), 3))\n","avg_neig_deg = nx.average_neighbor_degree(G, nodes=test_domains)\n","for i in range(len(test_domains)):\n","    Xtest[i,0] = G.in_degree(test_domains[i])\n","    Xtest[i,1] = G.out_degree(test_domains[i])\n","    Xtest[i,2] = avg_neig_deg[test_domains[i]]"]},{"cell_type":"markdown","metadata":{"id":"IjrhcQrzoj6d"},"source":["#### Create dataframes contain graph and text info"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713897637317,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"O4qbcN_jooYe"},"outputs":[],"source":["import pandas as pd\n","trains = pd.DataFrame({'domain_name':train_domains, 'in_deg':X_train[:,0], 'out_deg':X_train[:,1], 'avg_neig_deg':X_train[:,2], 'text':train_data, 'target':y_train})\n","tests = pd.DataFrame({'domain_name':test_domains, 'in_deg':Xtest[:, 0], 'out_deg':Xtest[:,1], 'avg_neig_deg':Xtest[:, 2], 'text':test_data})"]},{"cell_type":"markdown","metadata":{"id":"Z285kXgEK799"},"source":["#### Preprocessing\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1713897637317,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"V4-WWAkhK8As","outputId":"cb4f6310-2128-437b-9fb8-374e38142205"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/meizeus/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /home/meizeus/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/meizeus/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import pandas as pd\n","import re\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","\n","# initialize objects\n","stemmer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('greek'))\n","\n","def preprocess(contents):\n","  docs = []\n","\n","  for doc in contents:\n","\n","      document = re.sub(r'\\W', ' ', str(doc))# remove non-word (special) characters such as punctuation, numbers etc\n","      document = re.sub(r'\\s+br\\s+',' ', str(document)) # remove HTML <BR>\n","      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document) # remove all single characters\n","      document = re.sub(r'\\b\\d+\\b', ' ', document) # remove numbers\n","      document = re.sub(r'\\s+', ' ', document, flags=re.I) #re.I -> ignore case  and substitute multiple spaces with single space\n","      document = document.lower() # convert to Lowercase\n","      word_list = word_tokenize(document)# split the document based on whitespaces (--> List of words)\n","      word_list = [word for word in word_list if word not in (stop_words)]\n","      # Lemmatization\n","      # word_list = [stemmer.lemmatize(word) for word in word_list]\n","      document = ' '.join(word_list) # reconstruct the document by joining the words on each whitespace\n","      docs.append(document) # append all documents into a list 'docs'\n","      \n","  return pd.Series(docs)\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":196703,"status":"ok","timestamp":1713897834019,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"jCtm1tEzMNXm"},"outputs":[],"source":["# preprocess texts\n","trains['text'] = preprocess(trains['text'])\n","tests['text'] = preprocess(tests['text'])"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":["(605, 5)"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["tests.shape"]},{"cell_type":"markdown","metadata":{"id":"sTGfRM2dKyWJ"},"source":["#### Split data"]},{"cell_type":"markdown","metadata":{"id":"-lq_eRfflLpW"},"source":["### TOKENIZATION + NaN-removal"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":99951,"status":"ok","timestamp":1713898077063,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"OKE0i89UlLpX","outputId":"fc678587-8871-4b33-9a16-0c0f9044fbf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1506 entries, 0 to 1505\n","Data columns (total 7 columns):\n"," #   Column        Non-Null Count  Dtype  \n","---  ------        --------------  -----  \n"," 0   domain_name   1506 non-null   object \n"," 1   in_deg        1506 non-null   float64\n"," 2   out_deg       1506 non-null   float64\n"," 3   avg_neig_deg  1506 non-null   float64\n"," 4   text          1506 non-null   object \n"," 5   target        1506 non-null   object \n"," 6   tokens        1506 non-null   object \n","dtypes: float64(3), object(4)\n","memory usage: 82.5+ KB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>domain_name</th>\n","      <th>in_deg</th>\n","      <th>out_deg</th>\n","      <th>avg_neig_deg</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1038</th>\n","      <td>egnomi.gr</td>\n","      <td>240.0</td>\n","      <td>4.0</td>\n","      <td>200.75</td>\n","      <td>http www egnomi gr article kypello_eyboias_sfy...</td>\n","      <td>3</td>\n","      <td>[http, www, egnomi, gr, article, kypello_eyboi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     domain_name  in_deg  out_deg  avg_neig_deg  \\\n","1038   egnomi.gr   240.0      4.0        200.75   \n","\n","                                                   text target  \\\n","1038  http www egnomi gr article kypello_eyboias_sfy...      3   \n","\n","                                                 tokens  \n","1038  [http, www, egnomi, gr, article, kypello_eyboi...  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import word_tokenize\n","trains['tokens']  = trains['text'].apply(lambda x: word_tokenize(x, language='greek'))\n","trains['tokens'][:2]\n","\n","# filter out rows with empty lists in the 'tokens' column\n","trains = trains[trains['tokens'].apply(lambda x: len(x) > 0)]\n","trains = trains.reset_index(drop=True) # reset index of the filtered DataFrame\n","display(trains.info())\n","trains.sample()"]},{"cell_type":"markdown","metadata":{"id":"5kK-zgdIQWJx"},"source":["#### Train - Dev split"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1713898183493,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"Ik6W71-iK1Te","outputId":"ae2f5ccf-565d-4344-e356-c9b8216a2718"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train matrix dimensionality:  (1355, 6)\n","Dev matrix dimensionality:  (151, 6)\n","Test matrix dimensionality:  (605, 3)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","Xsplit = trains.drop(['target'], axis=1)\n","\n","# split data to train and dev\n","Xtrain, Xdev, ytrain, ydev = train_test_split(\n","    Xsplit,\n","    trains['target'],\n","    test_size=0.10, random_state=42)\n","\n","print(\"Train matrix dimensionality: \", Xtrain.shape)\n","print(\"Dev matrix dimensionality: \", Xdev.shape)\n","print(\"Test matrix dimensionality: \", Xtest.shape)  # This line has an issue\n"]},{"cell_type":"markdown","metadata":{"id":"EsOsaEFnlLpR"},"source":["### Imbalanced Dataset\n"]},{"cell_type":"markdown","metadata":{"id":"FrJCMdUglLpT"},"source":["**classes {0,7,6,4} are significantly smaller that majority class (class-3) and class-1 is rougly 25% of majority class**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cc6zc8wWlLpU","outputId":"eac3f80a-6ae0-43a4-9c3f-5427ae97870a"},"outputs":[{"data":{"text/plain":["Counter({'3': 548,\n","         '5': 280,\n","         '2': 263,\n","         '8': 189,\n","         '1': 140,\n","         '4': 113,\n","         '7': 99,\n","         '6': 98,\n","         '0': 82})"]},"metadata":{},"output_type":"display_data"}],"source":["from collections import Counter\n","import seaborn as sns\n","\n","c_counts = Counter(y_train)\n","display(c_counts)"]},{"cell_type":"markdown","metadata":{"id":"o07LR30tqMTv"},"source":["#### Encode targets (for NNs etc.)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1713898203155,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"E6LEl-UjlLpU","outputId":"13587d17-2d96-47ce-d6a4-7e15016486b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\tEncoded y_dev:\n","[[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n","\n","\n","\tEncoded y_dev:\n","[[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","encoder = OneHotEncoder(sparse_output=False)\n","\n","ytrain_en = encoder.fit_transform(np.array(ytrain).reshape(-1, 1)) # reshape y_train\n","ydev_en = encoder.fit_transform(np.array(ydev).reshape(-1, 1)) # reshape as y_dev\n","\n","print(\"\\tEncoded y_dev:\")\n","print(ydev_en[:2])\n","print(\"\\n\")\n","print(\"\\tEncoded y_dev:\")\n","print(ytrain_en[:2])"]},{"cell_type":"markdown","metadata":{"id":"uQp_FlAjlLpY"},"source":["### Fasttext"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"rPd3fleslLpY","outputId":"99457de0-166f-41b8-cad0-6152b96b1719"},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"]}],"source":["import fasttext\n","import gzip\n","\n","# m_path = '../../../../../big_files_DS/cc.el.300.bin.gz'\n","# with gzip.open(m_path, 'rb') as f:\n","#     model_data = f.read()\n","# ft_model = fasttext.load_model(model_data)\n","\n","ft_model = fasttext.load_model('../../../../../big_files_DS/cc.el.300.bin');"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"LnX6K7DclLpZ","outputId":"97460e38-6e4c-4018-f561-1c024e103f87"},"outputs":[{"data":{"text/plain":["(1355, 300)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# func to use fasttext to use vectorize text\n","def ft_vectorization(X_text):\n","    X_vec = []\n","\n","    for index, row in X_text.iterrows():\n","        text = row['text']\n","        vector = ft_model.get_sentence_vector(text) # row is sentence\n","        X_vec.append(vector)\n","\n","    return np.array(X_vec)\n","\n","# vectorize each text set\n","Xtrain_vec = ft_vectorization(Xtrain)\n","Xdev_vec = ft_vectorization(Xdev)\n","Xtest_vec = ft_vectorization(tests)\n","\n","np.array(Xtrain_vec).shape"]},{"cell_type":"markdown","metadata":{"id":"EdsSEu81lLpa"},"source":["### SMOTE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WsZPcJEtlLpb","outputId":"9db8f743-f72f-4fe4-f9b5-9948a59cb44e"},"outputs":[],"source":["# %%capture\n","# %pip install imblearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QaHpPHoKlLpb","outputId":"61de2462-4399-4cda-f309-4b5ecd9b6ca4"},"outputs":[{"name":"stdout","output_type":"stream","text":["(3591, 300) (3591,)\n"]}],"source":["# example implementation of SOTE technique\n","from imblearn.over_sampling import SMOTE\n","\n","method = SMOTE(k_neighbors = 3)\n","Xsmote, ysmote = method.fit_resample(Xtrain_vec, ytrain['target'])\n","\n","print(Xsmote.shape,ysmote.shape) # new shape"]},{"cell_type":"markdown","metadata":{"id":"NQOg66LRlLpd"},"source":["### TF-IDF vectorization"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"executionInfo":{"elapsed":90718,"status":"error","timestamp":1713898430799,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"},"user_tz":-180},"id":"8TnPlO81lLpd","outputId":"ebfc7ac8-1ba8-42e4-a010-7da997c513db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train matrix dimensionality:  (1355, 5000)\n","Dev matrix dimensionality:  (151, 5000)\n","Test matrix dimensionality:  (605, 5000)\n"]}],"source":["# create the training matrix. Each row corresponds to a domain name and each column to a word present in at least 10 webpages\n","# and at most 50 webpages of domain names. The value of each entry in a row is equal to the tf-idf weight of that word in the\n","# corresponding domain\n","vec = TfidfVectorizer(ngram_range=(1, 3),\n","                      decode_error='ignore',\n","                      max_features=5000,\n","                      strip_accents='unicode',\n","                      min_df=10, max_df=50)\n","\n","# vectorize the text sets\n","Xtrain_tf = vec.fit_transform(Xtrain['text'])\n","Xdev_tf = vec.transform(Xdev['text'])\n","Xtest_tf = vec.transform(tests['text'])\n","\n","print(\"Train matrix dimensionality: \", Xtrain_tf.shape)\n","print(\"Dev matrix dimensionality: \", Xdev_tf.shape)\n","print(\"Test matrix dimensionality: \", Xtest_tf.shape)"]},{"cell_type":"markdown","metadata":{"id":"T5skDsOelLpe"},"source":["### SVD for dimensionality reduction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vA67SgGlLpe"},"outputs":[],"source":["# reduce dimensionality using SVD\n","from sklearn.decomposition import TruncatedSVD\n","\n","svd = TruncatedSVD(n_components=400, random_state=4321)\n","Xtrain_svd = svd.fit_transform(Xtrain_tf)\n","Xdev_svd = svd.transform(Xdev_tf)\n","Xtest_svd = svd.transform(Xtest_tf)\n","\n","print(Xtrain_svd.shape, type(Xtrain_svd))\n","print(Xdev_svd.shape, type(Xdev_svd))\n","print(Xtest_svd.shape, type(Xtest_svd))"]},{"cell_type":"markdown","metadata":{"id":"oG-9Wrx8lLpe"},"source":["<h2 align='center'> Text - Logistic Regression (LR)</h2>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msul8FgqlLpf","outputId":"ebb03751-bec6-4f66-9c81-04d1fd986b0c"},"outputs":[],"source":["import optuna\n","def objective(trial, X, y):\n","    # define hyperparameters to optimize\n","    C = trial.suggest_float('C', 0.01, 100, log=True)\n","    penalty = trial.suggest_categorical('penalty', ['l2'])\n","    max_iter = trial.suggest_int('max_iter', 1000, 5000)\n","\n","    model = LogisticRegression(C=C, penalty=penalty, max_iter=max_iter, solver='liblinear')\n","\n","    # perform cross-validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n","\n","    # return average accuracy as the objective value\n","    return scores.mean()\n","\n","# begin studies\n","study = optuna.create_study(direction='maximize')\n","study.optimize(lambda trial: objective(trial, Xtrain_tf, ytrain), n_trials=30)\n","\n","best_params = study.best_params\n","best_score = study.best_value\n","\n","print(\"Best parameters:\", best_params)\n","\n","print(\"Best score:\", best_score)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"_ruf7sWRlLpq"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","# simple LR training && prediction(dev & test)\n","lr = LogisticRegression(max_iter=1500)\n","lr.fit(Xtrain_tf, ytrain)\n","ydev_lr = lr.predict_proba(Xdev_tf)\n","ytest_lr = lr.predict_proba(Xtest_tf)\n","\n","# cross-val LR training && prediction (dev & test)\n","lr_cv = LogisticRegression(**best_params)\n","lr_cv.fit(Xtrain_tf, ytrain)\n","\n","ydev_lrcv = lr_cv.predict_proba(Xdev_tf) # predict on dev\n","ytest_lrcv = lr_cv.predict_proba(Xtest_tf) # predict on test"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"y3Et9mlelLpr"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Entropy loss for Simple LR: 1.563823633828987\n","Cross Entropy loss for Cross-Val LR: 1.384521027088947\n"]}],"source":["# the dev set is -0.15 lower than test set on kaggle\n","\n","# cross-entropy loss for multi-class classification\n","def multiclass_cross_entropy(y_true, y_pred_prob):\n","    epsilon = 1e-15  # small value to prevent log(0)\n","    # clip predicted probabilities to avoid log(0)\n","    y_pred_prob = np.clip(y_pred_prob, epsilon, 1 - epsilon)\n","    # compute cross-entropy loss\n","    loss = -np.mean(np.sum(y_true * np.log(y_pred_prob), axis=1))\n","    return loss\n","\n","cross_entropy_loss1 = multiclass_cross_entropy(ydev_en, ydev_lr)\n","cross_entropy_loss2 = multiclass_cross_entropy(ydev_en, ydev_lrcv)\n","\n","print(f\"Cross Entropy loss for Simple LR: {cross_entropy_loss1}\")\n","print(f\"Cross Entropy loss for Cross-Val LR: {cross_entropy_loss2}\")\n"]},{"cell_type":"markdown","metadata":{"id":"o42zfr5flLpr"},"source":["<h2 align='center'> Text - Support Vector Machines (SVM)</h2>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEUo1-UklLps","outputId":"4931f1c4-c0f5-494c-ada6-e9cb93160328"},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","def objective(trial, X, y):\n","    # Define hyperparameters to optimize\n","    C = trial.suggest_float('C', 0.01, 10, log=True)\n","    kernel = trial.suggest_categorical('kernel', ['rbf', 'poly', 'sigmoid'])\n","    gamma = trial.suggest_float('gamma', 0.01, 100, log=True)\n","\n","    # Create SVM model with hyperparameters\n","    model = SVC(C=C, kernel=kernel, gamma=gamma)\n","\n","    # Perform cross-validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n","\n","    # Return average accuracy as the objective value\n","    return scores.mean()\n","\n","# Assuming X_train and y_train are defined elsewhere\n","study = optuna.create_study(direction='maximize')\n","study.optimize(lambda trial: objective(trial, Xtrain_tf, ytrain), n_trials=40)\n","\n","# Get the best parameters found during optimization\n","best_params = study.best_params"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"AXbpivv3lLps","outputId":"afcd242b-dd83-417e-aa4a-12750e94159f"},"outputs":[],"source":["\n","# simple SVC training && prediction(dev & test)\n","svc = SVC(probability=True)\n","svc.fit(Xtrain_tf, ytrain)\n","ydev_svc= svc.predict_proba(Xdev_tf)\n","ytest_svc = svc.predict_proba(Xtest_tf)\n","\n","# cross-val SVC training && prediction (dev & test)\n","best_svc= SVC(**best_params, probability=True)\n","best_svc.fit(Xtrain_tf, ytrain)\n","\n","ydev_b_svc = best_svc.predict_proba(Xdev_tf) # predict on dev\n","ytest_b_svc = best_svc.predict_proba(Xtest_tf) # predict on test\n","\n","# calculate cross entropy\n","cross_entropy_loss1 = multiclass_cross_entropy(ydev_en, ydev_svc)\n","cross_entropy_loss2 = multiclass_cross_entropy(ydev_en, ydev_b_svc)\n","\n","\n","print(f\"Cross Entropy loss for Simple SVC: {cross_entropy_loss1}\")\n","print(f\"Cross Entropy loss for Cross-Val SVC: {cross_entropy_loss2}\")"]},{"cell_type":"markdown","metadata":{"id":"4ewb21QhlLpt"},"source":["<h2 align='center'> Text - Random Forests Classifier (RFC) </h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOPRmr8RlLpt","outputId":"e4ae8d25-ba32-4612-a3ac-ae0dfa862de1"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","def objective(trial, X, y):\n","    # define hyperparameters to optimize\n","    n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n","    max_depth = trial.suggest_int('max_depth', 3, 100)\n","    min_samples_split = trial.suggest_int('min_samples_split', 2, 50)\n","    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n","\n","    # create Random Forest model with hyperparameters\n","    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n","                                   min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n","\n","    # perform cross-validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n","\n","\n","    return scores.mean()\n","\n","study = optuna.create_study(direction='maximize')\n","study.optimize(lambda trial: objective(trial, Xsmote, ysmote),\n","                n_trials=20)\n","\n","best_params = study.best_params # best parameters found"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65PEt1KtlLpu","outputId":"40d8abe0-76f4-43dd-ab3c-f381ceef0518"},"outputs":[],"source":["# simple RFC training && prediction(dev)\n","rfc = RandomForestClassifier()\n","rfc.fit(Xsmote, ysmote)\n","ydev_rfc = rfc.predict_proba(Xdev_vec)\n","ytest_rfc = rfc.predict_proba(Xtest_vec)\n","\n","# cross-val RFC training && prediction (dev & test)\n","best_rfc = RandomForestClassifier(**best_params)\n","best_rfc.fit(Xsmote, ysmote)\n","\n","ydev_b_rfc = best_rfc.predict_proba(Xdev_vec) # predict on dev\n","ytest_b_rfc = best_rfc.predict_proba(Xtest_vec) # predict on test\n","\n","\n","cross_entropy_loss1 = multiclass_cross_entropy(ydev_en, ydev_rfc)\n","cross_entropy_loss2 = multiclass_cross_entropy(ydev_en, ydev_b_rfc)\n","\n","print(f\"Cross Entropy loss for Simple RFC: {cross_entropy_loss1}\")\n","print(f\"Cross Entropy loss for Cross-Val RFC: {cross_entropy_loss2}\")"]},{"cell_type":"markdown","metadata":{"id":"Nt7TnUG5lLpv"},"source":["<h2 align='center'> Text - Extreme Gradient Boosting (XGB) </h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LaL0dmSLlLpv"},"outputs":[],"source":["%pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3hDVkfYlLpv"},"outputs":[],"source":["import xgboost as xgb\n","import optuna\n","from sklearn.model_selection import KFold, cross_val_score\n","\n","def objective(trial, X, y):\n","    # hyperparameters to optimize\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n","        'max_depth': trial.suggest_int('max_depth', 3, 10),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n","        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n","        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n","        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n","        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n","    }\n","\n","    # XGBoost model with hyperparameters\n","    model = xgb.XGBClassifier(**params)\n","\n","    # cross-validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n","\n","    return scores.mean()\n","\n","\n","study = optuna.create_study(direction='maximize')\n","study.optimize(lambda trial: objective(trial, Xtrain_tf, ytrain.astype(int)),\n","               n_trials=10)\n","\n","best_params = study.best_params  # best params\n"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"FWjlS-O4lLpw","outputId":"0630b514-4572-4c20-da89-4e41e2dd1ece"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Entropy loss for Simple XGB: 1.37209509138445\n","Cross Entropy loss for Cross-Val XGB: 1.1555710756852113\n"]}],"source":["# simple XGB training && prediction(dev)\n","Xbg = xgb.XGBClassifier()\n","Xbg.fit(Xtrain_tf, ytrain.astype(int))\n","ydev_xgb= Xbg.predict_proba(Xdev_tf)\n","ytest_xgb = Xbg.predict_proba(Xtest_tf)\n","\n","# cross-val XGB training && prediction (dev & test)\n","best_xgb = xgb.XGBClassifier(**best_params)\n","best_xgb.fit(Xtrain_tf, ytrain.astype(int))\n","\n","ydev_b_xgb = best_xgb.predict_proba(Xdev_tf) # predict on dev\n","ytest_b_xgb = best_xgb.predict_proba(Xtest_tf) # predict on test\n","\n","\n","cross_entropy_loss1 = multiclass_cross_entropy(ydev_en, ydev_xgb)\n","cross_entropy_loss2 = multiclass_cross_entropy(ydev_en, ydev_b_xgb)\n","\n","print(f\"Cross Entropy loss for Simple XGB: {cross_entropy_loss1}\")\n","print(f\"Cross Entropy loss for Cross-Val XGB: {cross_entropy_loss2}\")"]},{"cell_type":"markdown","metadata":{"id":"i-_CYXaclLpx"},"source":["<h2 align='center'> Text - Ada Boosting Classifier (Ada) </h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjhXgvJulLpx","outputId":"85ee69d2-7c7a-472c-c65b-7cb4448c7b13"},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.model_selection import KFold, cross_val_score\n","import optuna\n","\n","def objective(trial, X, y):\n","    # Hyperparameters to optimize\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n","        'algorithm': 'SAMME'\n","    }\n","\n","    # AdaBoost model with hyperparameters\n","    model = AdaBoostClassifier(**params, random_state=42)\n","\n","    # Cross-validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n","\n","    return scores.mean()\n","\n","study = optuna.create_study(direction='maximize')\n","study.optimize(lambda trial: objective(trial, Xtrain_tf, ytrain.astype(int)),\n","               n_trials=15)\n","\n","best_params = study.best_params  # Best params"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"BhAoV9XLlLpy","outputId":"fab978ab-dd1c-45d9-d13f-ea6d7163eb33"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Entropy loss for Simple ADA: 2.191959877410625\n","Cross Entropy loss for Cross-Val ADA: 2.1753044606631042\n"]}],"source":["import xgboost as xgb\n","# simple ADA training && prediction(dev)\n","ada = AdaBoostClassifier(algorithm='SAMME')\n","ada.fit(Xtrain_tf, ytrain.astype(int))\n","ydev_ada= ada.predict_proba(Xdev_tf)\n","ytest_ada = ada.predict_proba(Xtest_tf)\n","\n","# cross-val ADA training && prediction (dev & test)\n","best_ada = AdaBoostClassifier(**best_params, algorithm='SAMME')\n","best_ada.fit(Xtrain_tf, ytrain.astype(int))\n","\n","ydev_b_ada = best_ada.predict_proba(Xdev_tf) # predict on dev\n","ytest_b_ada = best_ada.predict_proba(Xtest_tf) # predict on test\n","\n","cross_entropy_loss1 = multiclass_cross_entropy(ydev_en, ydev_ada)\n","cross_entropy_loss2 = multiclass_cross_entropy(ydev_en, ydev_b_ada)\n","\n","print(f\"Cross Entropy loss for Simple ADA: {cross_entropy_loss1}\")\n","print(f\"Cross Entropy loss for Cross-Val ADA: {cross_entropy_loss2}\")"]},{"cell_type":"markdown","metadata":{},"source":["<h2 align='center'> Text - Gaussian Naive Bayes (NB) </h2>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import KFold, cross_val_score\n","import optuna\n","\n","def objective(trial, X, y):\n","    # Hyperparameters to optimize\n","    params = {\n","        'var_smoothing': trial.suggest_float('var_smoothing', 1e-10, 1e-5, log=True)\n","    }\n","\n","    # Naive Bayes model with hyperparameters\n","    model = GaussianNB(**params)\n","\n","    # Cross-validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n","\n","    return scores.mean()\n","\n","study = optuna.create_study(direction='maximize')\n","study.optimize(lambda trial: objective(trial, Xtrain_tf.toarray(), ytrain),\n","               n_trials=1000)\n","\n","best_params = study.best_params  # Best params\n"]},{"cell_type":"code","execution_count":137,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Entropy loss for Simple ADA: 23.33082908795291\n","Cross Entropy loss for Cross-Val ADA: 21.04349290285949\n"]}],"source":["# simple ADA training && prediction(dev)\n","nb = GaussianNB()\n","nb.fit(Xtrain_tf.toarray(), ytrain.astype(int))\n","ydev_nb= nb.predict_proba(Xdev_tf.toarray())\n","ytest_nb = nb.predict_proba(Xtest_tf.toarray())\n","\n","# cross-val ADA training && prediction (dev & test)\n","best_nb = GaussianNB(**best_params)\n","best_nb.fit(Xtrain_tf.toarray(), ytrain.astype(int))\n","\n","ydev_b_nb = best_nb.predict_proba(Xdev_tf.toarray()) # predict on dev\n","ytest_b_nb = best_nb.predict_proba(Xtest_tf.toarray()) # predict on test\n","\n","\n","cross_entropy_loss1 = multiclass_cross_entropy(ydev_en, ydev_nb)\n","cross_entropy_loss2 = multiclass_cross_entropy(ydev_en, ydev_b_nb)\n","\n","print(f\"Cross Entropy loss for Simple ADA: {cross_entropy_loss1}\")\n","print(f\"Cross Entropy loss for Cross-Val ADA: {cross_entropy_loss2}\")"]},{"cell_type":"markdown","metadata":{},"source":["#### ==================================== NEURAL NETS ====================================================="]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","import keras\n","from keras.layers import InputLayer\n","from keras.layers import Dropout, Dense, Input,BatchNormalization\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n","from keras.metrics import CategoricalCrossentropy\n","import keras_tuner as kt\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"markdown","metadata":{},"source":["#### Mini dataset for node graph"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 1355 entries, 203 to 1126\n","Data columns (total 6 columns):\n"," #   Column        Non-Null Count  Dtype  \n","---  ------        --------------  -----  \n"," 0   domain_name   1355 non-null   object \n"," 1   in_deg        1355 non-null   float64\n"," 2   out_deg       1355 non-null   float64\n"," 3   avg_neig_deg  1355 non-null   float64\n"," 4   text          1355 non-null   object \n"," 5   tokens        1355 non-null   object \n","dtypes: float64(3), object(3)\n","memory usage: 106.4+ KB\n"]}],"source":["Xtrain.info()\n","Xtrain_g = Xtrain.drop(['domain_name','text','tokens'], axis=1)\n","Xdev_g = Xdev.drop(['domain_name','text','tokens'], axis=1)\n","Xtest_g = tests.drop(['domain_name','text'], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["#### Hyperparameter tuning using Keras Tuner library"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score, recall_score, precision_score\n","\n","class Metrics(Callback):\n","    def __init__(self, valid_data):\n","        super(Metrics, self).__init__()\n","        self.validation_data = valid_data\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        logs = logs or {}\n","        val_predict_proba = self.model.predict(self.validation_data[0])\n","        val_predict = (val_predict_proba > 0.5).astype(int)  # convert probabilities to classes\n","\n","        val_targ = self.validation_data[1]\n","\n","        _val_f1 = f1_score(val_targ, val_predict, average=\"binary\")\n","        _val_recall = recall_score(val_targ, val_predict, average=\"binary\")\n","        _val_precision = precision_score(val_targ, val_predict, average=\"binary\")\n","\n","        logs['val_f1'] = _val_f1\n","        logs['val_recall'] = _val_recall\n","        logs['val_precision'] = _val_precision\n","        print(\" — val_f1: %.3f — val_precision: %.3f — val_recall: %.3f\" % (_val_f1, _val_precision, _val_recall))\n"]},{"cell_type":"markdown","metadata":{},"source":["#### KERAS TUNER"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 50 Complete [00h 00m 14s]\n","val_categorical_crossentropy: 1.992177963256836\n","\n","Best val_categorical_crossentropy So Far: 1.8619556427001953\n","Total elapsed time: 00h 17m 40s\n"]}],"source":["def build_model(hp):\n","    model = Sequential()\n","\n","    layer_index = 0\n","    for i in range(hp.Int(name='num_layers',min_value=1,max_value=5)):\n","        if layer_index == 0:\n","            model.add(Dense(hp.Int(name='hidden_units_'+str(i),min_value=128,max_value=1000,step=64),\n","                            activation=hp.Choice(name='activation_layer'+str(i),values=['relu','tanh','sigmoid']),\n","                            input_dim=Xtrain_g.shape[1]\n","                           ))\n","            model.add(Dropout(hp.Choice(name='dropout_layer_'+str(i),values=[0.1,0.2,0.3,0.4,0.5])))\n","        else:\n","            model.add(Dense(hp.Int(name='hidden_units_'+str(i),min_value=128,max_value=512,step=64),\n","                            activation=hp.Choice(name='activation_layer'+str(i),values=['relu','tanh'])))\n","            model.add(Dropout(hp.Choice(name='dropout_layer_'+str(i),values=[0.1,0.2,0.3,0.4,0.5])))\n","\n","        layer_index += 1\n","\n","    # add last layer that produces the logits\n","    model.add(Dense(9,  activation='softmax'))\n","\n","    # tune the learning rate for the optimizer\n","    # choose an optimal value from 0.01, 0.001, or 0.0001\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n","    model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=hp_learning_rate),\n","                  metrics=[CategoricalCrossentropy()])\n","\n","    return model\n","\n","# begin the tuning\n","tuner = kt.RandomSearch(build_model,\n","                        objective=kt.Objective('val_categorical_crossentropy',\n","                                               direction='min'),\n","                        max_trials=50,\n","                        directory='../../../../../data_challenge/new_KT_dir',\n","                        project_name='KT_tuning')\n","early_stopping = EarlyStopping(\n","    monitor='val_loss', patience=7)\n","tuner.search_space_summary()\n","\n","tuner.search(Xtrain_g, ytrain_en,\n","             validation_data=(Xdev_g, ydev_en), epochs=1000, batch_size = 64,\n","             callbacks=[early_stopping])"]},{"cell_type":"markdown","metadata":{},"source":["#### Get the five best MLPs (trained on graph-data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# after the end of the tuning get the best models parameters\n","tuner.results_summary();\n","# tuner.get_best_hyperparameters()[0].values\n","\n","# get the 4 best models evaluated by the tuner\n","mlp1 = tuner.get_best_models(num_models=5)[0];\n","mlp2 = tuner.get_best_models(num_models=5)[1];\n","mlp3 = tuner.get_best_models(num_models=5)[2];\n","mlp4 = tuner.get_best_models(num_models=5)[3];\n","mlp5 = tuner.get_best_models(num_models=5)[4];\n","\n","\n","# predict test graph\n","mlptest1 = mlp1.predict(Xtest_g) ;\n","mlptest2 = mlp2.predict(Xtest_g) ;\n","mlptest3 = mlp3.predict(Xtest_g) ;\n","mlptest4 = mlp4.predict(Xtest_g) ;\n","mlptest5 = mlp5.predict(Xtest_g) ;\n","\n","\n","# predict dev graph\n","mlpdev1 = mlp1.predict(Xdev_g) ;\n","mlpdev2 = mlp2.predict(Xdev_g) ;\n","mlpdev3 = mlp3.predict(Xdev_g) ;\n","mlpdev4 = mlp4.predict(Xdev_g) ;\n","mlpdev5 = mlp5.predict(Xdev_g) ;\n"]},{"cell_type":"markdown","metadata":{"id":"3aOcp-UFlLpy"},"source":["### ENSEMBLES - (soft voting) graph+text preds"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"data":{"text/plain":["((605, 9),\n"," (605, 9),\n"," (605, 9),\n"," (151, 9),\n"," (151, 9),\n"," (151, 9),\n"," (151, 9),\n"," (151, 9))"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["ytest_lrcv.shape,ytest_b_svc.shape,ytest_svc.shape,mlptest1.shape,mlptest2.shape,mlptest3.shape, mlptest4.shape,mlptest5.shape"]},{"cell_type":"code","execution_count":138,"metadata":{"id":"FvkaIl4AlLpy","outputId":"b168fbc8-bec7-4acf-e11a-6f8b58e8dba4"},"outputs":[{"data":{"text/plain":["(151, 9)"]},"execution_count":138,"metadata":{},"output_type":"execute_result"}],"source":["# -- combine probabilities of all the models we want\n","probs_devg = np.stack([ydev_lrcv,\n","                       ydev_svc,ydev_b_svc,\n","                       ydev_b_xgb,ydev_xgb,\n","                       ydev_b_nb,mlpdev1,mlpdev2\n","                       ], axis=2)\n","\n","avg_probs_devg = np.mean(probs_devg, axis=2) # average probability for each class all clfs\n","avg_probs_devg.shape"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[{"data":{"text/plain":["1.1992686349915866"]},"execution_count":139,"metadata":{},"output_type":"execute_result"}],"source":["ensemble_cross_en = multiclass_cross_entropy(ydev_en, avg_probs_devg)\n","ensemble_cross_en"]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[{"data":{"text/plain":["(605, 9)"]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"source":["### ----- Ensebles prediction on test ----------\n","\n","all_probs_stacked = np.stack([ytest_lrcv,\n","                       ytest_svc,ytest_b_svc,\n","                       ytest_b_xgb,ytest_xgb,ytest_b_nb], axis=2)\n","\n","# Calculate the average probability for each class across all classifiers\n","avg_probs_test = np.mean(all_probs_stacked, axis=2)\n","\n","avg_probs_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4RHW6rPzlLp0","outputId":"c69a06fd-3943-49f7-daed-070374943573"},"outputs":[{"data":{"text/plain":["(605, 9)"]},"execution_count":252,"metadata":{},"output_type":"execute_result"}],"source":["\n","all_probs_stacked = np.stack([ytest_lrcv,ytest_rfc,ytest_b_svc, ytest_b_xgb], axis=2)\n","\n","# Calculate the average probability for each class across all classifiers\n","average_probs = np.mean(all_probs_stacked, axis=2)\n","\n","average_probs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1MuXL2wlLp1"},"outputs":[],"source":["# write predictions to a file\n","with open('../../../../../text_sample_new_submission.csv', 'w') as csvfile:\n","    writer = csv.writer(csvfile, delimiter=',')\n","    lst = list()\n","    for i in range(9):\n","        lst.append('class_'+str(i))\n","    lst.insert(0, \"domain_name\")\n","    writer.writerow(lst)\n","    for i,test_domain in enumerate(test_domains):\n","        lst = average_probs[i,:].tolist()\n","        # print(lst)\n","        lst.insert(0, test_domain)\n","        writer.writerow(lst)"]}],"metadata":{"colab":{"collapsed_sections":["EdsSEu81lLpa"],"provenance":[]},"kernelspec":{"display_name":"tf_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
